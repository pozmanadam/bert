{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "#import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7023366, 'Fordította:\\n')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'SZTAKI-HLT/hubert-base-cc' # bert-base-multilingual-cased, SZTAKI-HLT/hubert-base-cc\n",
    "\n",
    "\n",
    "data_path = f\"C:/Users/pozmanreka/PycharmProjects/test/venv/src/neural_punctuator/data/\"\n",
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "file_path = data_path + \"hu.txt\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.readlines()\n",
    "    \n",
    "len(text), text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3288415"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "text = list(OrderedDict.fromkeys(text))\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eleget láttam!\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "text = shuffle(text, random_state=0)\n",
    "\n",
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    text = text.replace('_', ' ')\n",
    "    text = text.replace('\\\\', ' per ')\n",
    "\n",
    "    \n",
    "    reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    r = re.compile(reg, re.DOTALL)\n",
    "    text = r.sub(' ', text)\n",
    "    \n",
    "    text = text.replace('-', ',')\n",
    "    text = text.replace(';', '.')\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('(', ', ')\n",
    "    text = text.replace(')', ', ')\n",
    "    \n",
    "    text = text.replace('...', '.')\n",
    "    text = text.replace('.\\\"', ',')\n",
    "    text = text.replace('\"', ',')\n",
    "    \n",
    "    text = re.sub(r'\\s-\\s', ' , ', text)\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text)   \n",
    "    \n",
    "    text = re.sub(r',\\s?,', ', ', text)\n",
    "    text = re.sub(r'\\.[\\s+.]+', '. ', text)\n",
    "    \n",
    "    text = re.sub(r',\\s?,', ',', text)\n",
    "    \n",
    "    remove_space_before = [',', '?', '.', '!', '\\n']\n",
    "    for c in remove_space_before:\n",
    "        text = text.replace(' ' + c, c)\n",
    "        \n",
    "    text = re.sub(r'(\\w)(\\n)', r'\\1.\\2', text)\n",
    "    \n",
    "    text = re.sub(r',\\s+[\\.,\\s]+', ', ', text)\n",
    "    text = re.sub(r'\\.\\s+[\\.,\\s]+', '. ', text)\n",
    "    text = re.sub(r'\\.[\\.,]+', '.', text)\n",
    "    text = re.sub(r'\\,[\\.,]+', ',', text)\n",
    "    text = re.sub(r'\\,[\\.,]+', ',', text)\n",
    "    text = text.lstrip('.,?')\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3288030, 'eleglátta')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [clean_text(t) for t in text]\n",
    "text = [t for t in text if len(t) > 0]\n",
    "len(text), text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join(text[train_n:train_n+valid_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gyűössz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[926886]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94308 ebrazillse\n",
      "582550 hotetsza 38,brazil?\n",
      "599521 brazili\n",
      "798321 a braziloknál ovzic\n",
      "926804 brazili[a-za-z].\n",
      "1047346 noéppbraziliábvoltués ő.\n",
      "1881268 brazilveszteolszokkszembe\n",
      "2466235 a brazilróapssebészntartmagát?\n",
      "2632471 mindbizonnynéltúl átkelébraziliából\n",
      "2746731 a hidfalklaáramlitalálkoza melbrazilla\n",
      "2822191 venezuelábabraziliábabolíviábajamaicábaés a gaboköztársaságba\n",
      "2986246 a brazilifrontellenállás vezetői hatalmkudarcszenvedtemivelfogtés kivégeztszámmagranvezetőt a magroszászlóaljbó\n",
      "3031531 ezbraziliesőerdő,nesze\n",
      "3205420 istenea brazila legszeembere\n",
      "3210125 a brazilítapí\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(text):\n",
    "    if 'brazil' in t:\n",
    "        print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4575, 8308, 3576, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\".?,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 4575, '?': 8308, ',': 3576}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \".?,\"}\n",
    "target_token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4575, 8308, 3576]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids = list(target_token2id.values())\n",
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "\n",
    "def create_target(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = text.split(' ')\n",
    "\n",
    "    for word in tqdm(words):\n",
    "        target = 0\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word.endswith(target_token):\n",
    "                word = word.rstrip(target_token)\n",
    "                target = id2target[target_id]\n",
    "\n",
    "        encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        for w in encoded_word:\n",
    "            encoded_words.append(w)\n",
    "        for _ in range(len(encoded_word)-1):\n",
    "            targets.append(-1)\n",
    "        targets.append(target)\n",
    "#         print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "        assert(len(encoded_word)>0)\n",
    "\n",
    "    return encoded_words, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397846, 37823, 15257)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n = 210_000\n",
    "valid_n = 20_000\n",
    "test_n = 8_000\n",
    "\n",
    "train_text = ' '.join(text[:train_n])\n",
    "valid_text = ' '.join(text[train_n:train_n+valid_n])\n",
    "test_text = ' '.join(text[train_n+valid_n:test_n+valid_n+train_n])\n",
    "\n",
    "len(train_text.split(' ')), len(valid_text.split(' ')), len(test_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3058030"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) - train_n - valid_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hangoda',\n",
       " 'lefordította',\n",
       " 'vésete',\n",
       " 'ritk',\n",
       " 'ntudhasználerőime',\n",
       " 'jobbmivelem?',\n",
       " 'nöltmeszóvsegíthrajt',\n",
       " 'kepusztítan',\n",
       " 'és',\n",
       " 'mcsinál']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.split(' ')[154349-5:154349+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf052e492a54946b821e0a4573fd999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c0474690788c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalid_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-1ddc00b6bbe6>\u001b[0m in \u001b[0;36mcreate_target\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#         print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mencoded_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_tokens, train_targets = create_target(train_text)\n",
    "valid_tokens, valid_targets = create_target(valid_text)\n",
    "test_tokens, test_targets = create_target(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For backward campatibility\n",
    "train_tokens, train_targets = [train_tokens], [train_targets]\n",
    "valid_tokens, valid_targets = [valid_tokens], [valid_targets]\n",
    "test_tokens, test_targets = [test_tokens], [test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "with open(data_path + f'{model_type}/train_data.pkl', 'wb') as f:\n",
    "    pickle.dump((train_tokens, train_targets), f)\n",
    "with open(data_path + f'{model_type}/valid_data.pkl', 'wb') as f:\n",
    "    pickle.dump((valid_tokens, valid_targets), f)\n",
    "with open(data_path + f'{model_type}/test_data.pkl', 'wb') as f:\n",
    "    pickle.dump((test_tokens, test_targets), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for ds in (train_targets, valid_targets, test_targets):\n",
    "    c = Counter((t for targets in ds for t in targets))\n",
    "    print('\\t'.join([str(c[i]) for i in (1,2,3,0,-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = []\n",
    "i = 0\n",
    "\n",
    "raw_words = valid_text.split(' ')\n",
    "\n",
    "for te, ta in zip(valid_tokens[0], valid_targets[0]):\n",
    "    if ta == -1:\n",
    "        e.append(te)\n",
    "    else:\n",
    "        e.append(te)\n",
    "        print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t{raw_words[i]}\")\n",
    "        e = []\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(valid_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(text[train_n:train_n+valid_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
